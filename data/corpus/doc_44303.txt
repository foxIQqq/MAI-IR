Выпуск распределённого хранилища Ceph 10.2.0

Компания Red Hat представила релиз проекта Ceph 10.2.0, предоставляющего инфраструктуру для создания объектных хранилищ, блочных устройств и файловых систем, распределенных по тысячам машин и способных хранить эксабайты данных. Выпуск позиционируется как значительный релиз, который послужит основой для формирования новой ветки с длительным циклом поддержки (LTS). Пакеты сформированы для CentOS 7.x, Debian Jessie 8.x, Ubuntu 14.04/16.04 и Fedora 22+. Ceph позволяет добиться очень высокого уровня отказоустойчивости хранилища, благодаря отсутствию единой точки отказа и репликации с хранением нескольких экземпляров данных на разных узлах. Обработку данных и метаданных выполняют различные группы узлов в кластере. Поддерживается создание снапшотов, динамическое выделения места в хранилище, многослойное хранение (редко используемые данные на НЖМД, а часто используемые на SSD), самодиагностика и самоконфигурирование. При добавлении или удалении новых узлов, массив данных автоматически ребалансируется с учетом изменения конфигурации. Выпуск Ceph 10.2.0 примечателен стабилизацией реализации POSIX-совместимой файловой системы CephFS. Как и в случае блочного устройства RBD, файловая система CephFS разворачивается поверх распределённого кластера хранения Ceph, включающего как минимум один сервер метаданных. Предлагается два варианта клиентской части файловой системы CephFS: модуль ядра Linux и реализация в пространстве пользователя через подсистему FUSE. В связи с переводом CephFS в разряд стабильных подсистем, некоторые возможности теперь отключены по умолчанию, например, недоступны снапшоты и конфигурация с несколькими активными серверами метаданных, которые пока не рекомендуются для промышленного применения. До полнофункционального состояния доведена утилита для восстановления целостности ФС после сбоя. В состав включен новый модуль cephfs-volume-manager, позволяющий управлять хранилищами для OpenStack. Добавлена экспериментальная поддержка развёртывания нескольких ФС в одном кластере. В реализации блочного устройства RBD (Ceph Block Device) добавлена поддержка зеркалирования разделов (асинхронной репликации) с привлечением нескольких разных кластеров хранения. Репликация выполнена через трансляцию в другой кластер журнала изменений и может использоваться для организации зеркала в территориально разнесённом хранилище, доступном через глобальную сеть. Из других новшеств отмечается поддержка динамического управления включением таких возможностей, как эксклюзивные блокировки, object-map, fast-diff и журналирование. Добавлена возможность переименования снапшотов RBD. Полностью переписан интерфейс командной строки, добавлена поддержка автодополнения ввода в bash. В объектном хранилище RADOS (Reliable Autonomic Distributed Object Store), позволяющем организовать хранение данных из приложений на различных языках программирования и служащем основой для RBD и CephFS, представлен новый более быстрый OSD-бэкенд BlueStore (Object Storage Device), который пока рассматривается как экспериментальная возможность, но в будущем запланирован для включения по умолчанию. Традиционно Ceph использует для хранения данных и метаданных локальные ФС узлов, такие как XFS и Btrfs, что приводит к лишнему усложнению и ограничивает производительность. BlueStore обеспечивает хранение на уровне прямого доступа к блочному устройству, используя лишь небольшую ФС для метаданных. Для большинства типовых нагрузок подобный подход позволяет добиться удвоения производительности. В RADOS Gateway, прослойке для организации доступа к объектному хранилищу RADOS через типовые RESTful API (Amazon S3, OpenStack Swift), переписана и перепроектирована система межкластерного взаимодействия, что позволило реализовать возможность создания active/active конфигураций и двунаправленного восстановления (bidirectional fail-over). Добавлена экспериментальная поддержка доступа к данным через NFS. Реализована поддержка протокола AWS4 и API OpenStack Keystone v3.