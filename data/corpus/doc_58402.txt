Исследование влияния AI-ассистентов, подобных GitHub Copilot, на безопасность кода

Группа исследователей из Стэнфордского университета изучила влияние использования интеллектуальных помощников при написании кода на появление в коде уязвимостей. Рассматривались решения на базе платформы машинного обучения OpenAI Codex, такие как GitHub Copilot, позволяющие формировать достаточно сложные блоки кода, вплоть до готовых функций. Опасения связаны с тем, что так как для тренировки модели машинного обучения использован реальный код из публичных репозиториев GitHub, в том числе содержащий уязвимости, синтезированный код может повторять ошибки и предлагать код, в котором присутствуют уязвимости, а также не учитывать необходимость выполнения дополнительных проверок при обработке внешних данных. К проведению исследования были привлечены 47 добровольцев, имеющих разный опыт в программировании - от студентов, до профессионалов с десятилетним опытом. Участники были разделены на две группы - экспериментальная (33 человека) и контрольная (14 человек). Обе группы имели доступ к любым библиотекам и интернет-ресурсам, в том числе могли использовать готовые примеры со Stack Overflow. Экспериментальной группе была предоставлена возможность использования AI-ассистента. Каждому участнику было дано 5 заданий, связанных с написанием кода, в котором потенциально легко допустить ошибки, приводящие к уязвимостям. Например, были задания по написанию функций шифрования и расшифровки, использованию цифровых подписей, обработке данных, участвующих в формировании файловых путей или SQL-запросов, манипуляции с большими числами в коде на языке Си, обработке ввода, отображаемого в web-страницах. Для рассмотрения влияния языков программирования на безопасность кода, получаемого при использовании AI-ассистентов, задания охватывали языки Python, Си и JavaScript. В итоге было выявлено, что участники, использовавшие интеллектуальный AI-ассистент на базе модели codex-davinci-002, подготовили значительно менее безопасный код, чем участники, которые не пользовались AI-ассистентом. В общем виде только 67% участников группы, использовавшей AI-ассистент, смогли предоставить корректный и безопасный код, в то время как в другой группе этот показатель составил 79%. При этом показатели самооценки были обратными - участники пользовавшиеся AI-ассистентом считали, что их код будет более безопасным, чем у участников из другой группы. Кроме того, было отмечено, что участники, которые меньше доверяли AI-ассистенту и больше тратили времени на разбор выдаваемых подсказок и вносили в них изменения, допустили меньше уязвимостей в коде. Например, код скопированный из криптографических библиотек содержал более безопасные значения параметров по умолчанию, чем код, предложенный AI-ассистентом. Также при использовании AI-ассистента фиксировался выбор менее надёжных алгоритмов шифрования и отсутствие проверок подлинности возвращаемых значений. В задании, связанном с манипуляцией числами на языке Си, в коде, написанном с использованием AI-ассистента, было допущено больше ошибок, приводящих к целочисленному переполнению. Дополнительно можно отметить похожее исследование группы из Нью-Йоркского университета, проведённое в ноябре с привлечением 58 студентов, которым было предложено реализовать на языке Си структуру для обработки списка покупок. Полученные результаты показали незначительное влияние AI-ассистента на безопасность кода - пользователи, которые применяли AI-ассистент, в среднем допустили примерно на 10% больше ошибок, связанных с безопасностью.