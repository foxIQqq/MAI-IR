Выпуск распределенной файловой системы GlusterFS 3.5

Сообщество Gluster объявило о выходе распределенной файловой системы GlusterFS 3.5, позволяющей организовать работу распределённого на несколько узлов хранилища, развёртываемого поверх штатных POSIX ФС, таких как Ext4, XFS и Btrfs, с использованием механизма FUSE (ФС в пользовательском пространстве). GlusterFS предоставляет средства автоматического восстановления после сбоев и обеспечивает практически неограниченную масштабируемость, благодаря отсутствию привязки к централизованному серверу мета-данных (используются распределённые хэш-таблицы). Готовые для установки бинарные пакеты с GlusterFS 3.5 подготовлены для Fedora, RHEL, CentOS, Debian, openSUSE, SLES и Ubuntu. Основные новшества: Поддержка создания снапшотов отдельных файлов. С практической стороны данная возможность может быть востребована для создания снапшотов образов работающих виртуальных машин, что позволяет вернуть состояние виртуальной машины на момент в прошлом только силами GlusterFS без задействования дополнительных инструментов. В будущих выпусках ожидается поддержка создания снапшотов разделов; Возможность сжатия данных при передаче по сети, что позволяет сократить общую нагрузку на сеть при выполнении операций, инициированных клиентом. В зависимости от характера данных применение сжатия может существенно повысить производительность работы с разделами при незначительном увеличении нагрузки на CPU, особенно в ситуациях когда пропускная способность канала связи клиента является узким местом; Поддержка шифрования данных дисковых разделов на стороне сервера с использованием ключей, доступных только клиенту. Шифруется только содержимое файлов, имена и метаданные остаются незашифрованными. Кроме того, шифрование не применимо при использовании NFS; Автоматическое обнаружение отказа отдельного хранилища (Brick Failure Detection); Увеличено качество ведения логов, добавлено отображение дополнительной статистики и указание причин возникновения тех или иных событий, а также данных о том, какие файлы затрагивают данные события; Новый метод прямого доступа к данным через GFID, позволяющий обратиться к данным в трансляторе изменений; Уход от необходимости перезапуска NFS-сервера при изменении параметров раздела; Расширение числа возможных конфигураций квот от нескольких сотен до 65536 для раздела; Поддержка упреждающего чтения (readdir_ahead) списка директорий для увеличения производительности последовательного чтения директорий; Поддержка операции zerofill для заполнения нулями новых образов виртуальных машин; Переработана архитектура системы геораспределённой репликации (Geo-Replication). Если раньше процесс репликации gsyncd выступал в качестве единой точки отказа, так как он запускался на одном узле кластера, выход которого из строя сулил прекращение geo-репликации, то в новой реализации репликация выполняется на всех узлах, обслуживающих раздел (каждый узел теперь отвечает за синхронизацию хранимых на нём данных). Для определения изменившихся файлов, для которых необходимо выполнить синхронизацию, теперь используется журнал изменений (changelog xlator). Кроме rsync добавлена поддержка нового метода синхронизации "tar+ssh", который может быть востребован в конфигурациях с большим числом мелких файлов.