Представлена система синтеза изображений Stable Diffusion 2.0

Компания Stability AI опубликовала вторую редакцию системы машинного обучения Stable Diffusion, способной синтезировать и изменять изображения на основе предложенного шаблона или текстового описания на естественном языке. Код инструментов для обучения нейронной сети и генерации изображений написан на языке Python с использованием фреймворка PyTorch и опубликован под лицензией MIT. Уже обученные модели открыты под пермиссивной лицензией Creative ML OpenRAIL-M, допускающей использование в коммерческих целях. Ключевые улучшения в новой редакции Stable Diffusion: Создана новая модель синтеза изображений по текстовому описанию - SD2.0-v, поддерживающая генерацию изображений с разрешением 768x768. Новая модель обучена с использованием коллекции LAION-5B, включающей 5.85 миллиардов изображений с текстовыми описаниями. Модель использует тот же набор параметров как и у модели Stable Diffusion 1.5, но отличается переходом на использование принципиально иного кодировщика OpenCLIP-ViT/H, позволившего существенно повысить качество результирующих изображений. Подготовлен упрощённый вариант SD2.0-base, обученный на изображениях 256x256 с использованием классической модели предсказания шумов и поддерживающий генерацию изображений с разрешением 512x512. Предоставлена возможность использования технологии суперсэмплинга (Super Resolution) для увеличения разрешения исходного изображения без снижения качества, используя алгоритмы пространственного масштабирования и реконструкции деталей. Предоставленная модель обработки изображений (SD20-upscaler) поддерживает четырёхкратное увеличение масштаба, что позволяет формировать изображения с разрешением 2048x2048. Предложена модель SD2.0-depth2img, учитывающая глубину и пространственное расположение объектов. Для монокулярной оценки глубины используется система MiDaS. Модель позволяет синтезировать новые изображения, используя другое изображение в качестве шаблона, которые могут радикально отличаться от оригинала, но сохранять общую композицию и глубину. Например, можно использовать позу человека на фотографии для формирования другого персонажа в той же позе. Обновлена модель для модификации изображений - SD 2.0-inpainting, позволяющая при помощи текстовых подсказок заменять и изменять части изображения. Проведена оптимизация моделей для использования на обычных системах с одним GPU.