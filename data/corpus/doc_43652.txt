Релиз ядра Linux 4.4

После двух месяцев разработки Линус Торвальдс анонсировал релиз ядра Linux 4.4. Среди наиболее заметных изменений: задействование Direct I/O и AIO для примонтированных в loop-режиме ФС, устойчивый к сбоям RAID5 с дополнительным журналированием, поддержка поллинга для блочных устройств, драйвер для SSD-накопителей Open-Channel на основе памяти NVM, работа слушающих TCP-сокетов в неблокирующем режиме, новый системный вызов mlock2(), поддержка 3D в виртуальном GPU virtio-gpu, возможность запуска программ eBPF непривилегированными пользователями, поддержка постоянно работающих eBPF-программ, механизм определения потери TCP-пакетов RACK, KMS-драйвер для Raspberry Pi, xconfig переведён на Qt5. В новую версию принято около 13 тысяч исправлений от 1548 разработчиков, размер патча - 49 Мб (изменения затронули 10606 файлов, добавлено 714106 строк кода, удалено 471010 строк). Около 44% всех представленных в 4.4 изменений связаны с драйверами устройств, примерно 16% изменений имеют отношение к обновлению кода специфичного для аппаратных архитектур, 15% связано с сетевым стеком, 4% - файловыми системами и 3% c внутренними подсистемами ядра. 12.9% изменений внесено сотрудниками компании Intel, 5.2% - Samsung, 5.1% - Red Hat, 3.5% - Atmel, 3.5% - Linaro, 2.3% - IBM, 2.1% - Google, 2.0% - SUSE, 1.8% - ARM, 1.6% - Texas Instruments, 1.6% - Freescale, 1.4% - AMD, 1.3% - Oracle. Латиноамериканский Фонд свободного ПО опубликовал полностью свободный вариант ядра 4.4 - Linux-libre 4.4-gnu, очищенный от элементов прошивок и драйверов, содержащих несвободные компоненты или участки кода, область применения которых ограничена производителем. В новом выпуске проведены операции по чистке блобов в драйверах qed, fdp, nfcmrvl, rtl8xxxu и rohm_bu21023, улучшен код чистки блобов в загрузчике микрокода, удалён драйвер ft1000. Из наиболее интересных новшеств ядра Linux 4.4 можно отметить: Дисковая подсистема, ввод/вывод и файловые системы Для файловых систем, примонтированных в режиме "loopback" (монтирование из файла с использованием блочного устройства loop), реализована возможность использования прямых (Direct I/O) и асинхронных (AIO) операций ввода/вывода при чтении и записи в файл, связанный с loop-устройством. Использование Direct I/O при обращении к файлу с образом ФС позволяет избежать двойного кэширования, существенно сокращает потребление памяти и уменьшает число переключений контекста; В подсистему MD интегрированы наработки компании Facebook по увеличению живучести программного RAID5 в случае краха системы. Добавленный улучшения позволят реализовать режим журналирования RAID5, при котором на отдельном носителе (SSD или NVRAM) создаётся специальный журнал. Записываемые в RAID данные вначале сохраняются в журнале, а затем разносятся по входящим в RAID-массив дискам. Журнал позволяет гарантировать целостное состояние RAID в ситуациях неожиданного отключения питания, даже если RAID находился в деградированном состоянии. Если питание будет прервано на стадии когда составляющие транзакцию данные были записаны лишь на часть дисков, повреждения данных можно избежать так как вся информация о транзакции отражена в журнале. Журнал также позволяет повысить производительность некоторых операций и сократить задержки, но данные оптимизации пока не включены в состав ядра; Поддержка поллинга ввода/вывода для блочных устройств (I/O polling). Поллинг позволяет уменьшить нагрузку на систему при использовании высокопроизводительных устройств за счёт периодического опроса состояния вместо генерации прерываний. Как следствие, в определённых ситуациях включение поллинга позволяет существенно повысить пропускную способность и сократить задержки ввода/вывода. Включение производится через запись 1 в /sys/block/DEV/queue/io_poll. В настоящее время поддерживается только режим O_DIRECT, а реализация помечена как экспериментальная и предназначенная для тестирования; Реализована спецификация LightNVM, расширяющая драйвер NVM поддержкой SSD-накопителей, допускающих низкоуровневый прямой доступ к физическому носителю (например, первое поколение Open-Channel SSD-накопителей на основе памяти NVM). Для таких устройств ядро берёт на себя функции низкоуровневого управления хранилищем, которые в обычном Flash выполняются на уровне контроллера (FTL, Flash Translation Layer). LightNVM обеспечивает такие операции как управление размещением данных, сборка мусора и организация параллельного доступа. Функции управления сбойными блоками, атомарность ввода/вывода и размещение метаданных по-прежнему выполняются чипом накопителя; В клиент NFS добавлена поддержка операции CLONE, определённой в спецификации NFSv4.2 и позволяющей организовать быстрое копирование файлов, используя ioctl NFS_IOC_CLONE, реализованный по аналогии с BTRFS_IOC_CLONE; В Btrfs добавлена отладочная опция монтирования "fragment", установка которой приводит к излишней фрагментации данных и метаданных. Для RAID0/10/5/6 реализован фильтр балансировки разнесения групп блоков по дискам, позволяющий выборочно ребалансировать только блоки, не разнесённые на достаточное число устройств; В XFS добавлено отдельное накопление статистики для каждой файловой системы (/sys/fs/xfs/BLOCK/stats/stats) и реализован специальный файл /sys/fs/xfs/BLOCK/stats/stats_clear для очистки статистики. Глобальная статистика доступа в /proc и продублирована в /sys/fs/xfs/stats/stats; В CIFS реализована возможность выполнения операции копирования на стороне сервера (copy offload, CopyChunk) при копировании данных между разными разделами (share), размещёнными на одном сервере (ранее CopyChunk применялся только при копировании внутри одного раздела). Операция копирования на стороне сервера не требует перемещения данных по сети и производится до ста раз быстрее; В CIFS добавлены опции монтирования "nopersistenthandles" и "persistenthandles", управляющие включением расширений "persistent handles", предоставляющих средства для повышения доступности открытых файловых дескрипторов в кластерных конфигурациях. Также добавлена опция "resilienthandles", позволяющая снизить вероятность потери данных в случае сбоя при подключении к серверам без поддержки "persistent handles"; Для блочных устройств представлен интерфейс Persistent Reservations, позволяющий зарезервировать за определённой системой область в совместно используемых хранилищах; Сетевая подсистема Обработка слушающих TCP-сокетов (listen) полностью избавлена от установки блокировок (lockless). Тесты показывают, что после задействования неблокирующего режима производительность слушающих сокетов увеличилась на 2-3 порядка (!), например, один слушающий сокет теперь способен обработать 3.5 млн SYN-пакетов в секунду; В setsockopt() добавлена поддержка флага SO_INCOMING_CPU и расширена логика выбора CPU при использовании SO_REUSEPORT. SO_INCOMING_CPU позволяет организовать обработку в текущем процессе только тех пакетов, которые до этого были обработаны сетевым стеком на том же CPU. Закрепление привязанных к одному CPU RX-очередей и обработчиков слушающих сокетов позволяет более эффективно использовать процессорный кэш; Добавлен RACK - новый механизм определения потери TCP-пакетов, который в отличие от штатного метода определения факта потери пакета, отталкивается от времени передачи, а не последовательности прихода пакетов. Суть работы RACK в том, что при получении ACK-подтверждения для пакета, любые неподтверждённые пакеты, отправленные как минимум на RTT (round-trip time) раньше подтверждённого пакета, считаются потерянными и потребуют повторной отправки. Новый алгоритм уже протестирован в инфраструктуре Google и будет предложен для утверждения в качестве стандарта IETF; Память и системные сервисы Возможность загрузки программ eBPF непривилегированными пользователями для их использования в качестве фильтров для сокетов. Ранее из соображений безопасности доступ к системному вызову ebpf() был открыт только пользователю root. В текущей версии ядра код проверки корректности загружаемых программ был значительно улучшен и непривилегированным пользователям дана возможность запуска ограниченных по функциональности программ eBPF, которые могут использоваться для создания простых сетевых фильтров. Возможности eBPF по трассировке, классификации трафика и манипуляциям с данными ядра по-прежнему доступны только для root. Для запрета обращения к системному вызову ebpf() из непривилегированных процессов добавлен sysctl kernel.unprivileged_bpf_disabled; Возможность постоянного выполнения eBPF-программ и сопоставлений (Persistent eBPF maps/progs), продолжающих работу и после завершения процесса, инициировавшего их выполнение. Объекты выполняемого eBPF размещаются в области /sys/fs/bpf/ и могут совместно использоваться несколькими процессами. Например, таким способом удобно создавать классификаторы и обработчики трафика; Добавлен фреймворк "devfreq cooling" для управления температурным режимом устройств, позволяющий при наличии соответствующей аппаратной поддержки перевести перегревающееся устройство в режим пониженного энергопотребления для удержания температуры в заданных границах; Добавлен системный вызов mlock2(), расширяющий возможности системного вызова mlock() поддержкой дополнительного аргумента, позволяющего задействовать новый режим блокировки VM_LOCKONFAULT, при котором страницы памяти в указанном диапазоне будут закреплены в ОЗУ не сразу, а только после возникновения page fault (обращение к невыделенным страницам памяти); Изменено содержимое файлов "stat", размещённых в поддиректории каждого процесса в /proc (например, /proc/123/stat). Поле wchan (30 столбец), которое содержало абсолютный адрес, по которому был заблокирован процесс, могло использоваться злоумышленниками для получения важной информации о ядре. Отныне данное поле переведено в разряд флагов: содержит ноль для выполняемых процессов и единицу для заблокированных; Многочисленные улучшения в утилите perf. Например, perf теперь может собирать и загружать программы eBPF для решения задач мониторинга производительности и трассировки событий; Добавлен модуль userio с реализацией протокола, позволяющего в пространстве пользователя эмулировать устройства с последовательным портом ввода/вывода, такие как тачпады; Для систем x86 добавлен параметр конфигурации CONFIG_DEBUG_WX, при включении которого ядро будет выдавать предупреждения о маппинге секций памяти, одновременно помеченных доступными на запись и выполнение; Графический конфигуратор xconfig портирован на Qt5. Поддержка сборки xconfig с Qt3 прекращена; Виртуализация и безопасность В KVM и VFIO добавлена возможность обработки аппаратных прерываний в гостевой системе без проброса через прослойку, работающую на стороне хоста - прерывания от устройств PCI передаются напрямую в vCPU; Вложенная виртуализация в KVM теперь поддерживает VPID по аналогии с PCID, но для vCPU; В KVM добавлена поддержка разбиения кода контролера прерываний, при которой LAPIC реализуется в ядре, а IOAPIC/PIC/PIT в пространстве пользователя, что уменьшает подверженность гипервизора некоторым типам атак; В драйвер VMware balloon, позволяющий исключить дублирование идентичных областей памяти в разных виртуальных окружениях, добавлена возможность манипулирования страницами памяти размером 2 Мб, что значительно уменьшает накладные расходы на стороне гипервизора и гостевой системы при выполнении операций связывания (ballooning) и разделения (unballooning) общей памяти; Поддержка аппаратных генераторов случайных чисел ST Microelectronics; В ptrace добавлена поддержка формирования дампа задействованных для процесса фильтров seccomp (PTRACE_SECCOMP_GET_FILTER); Оборудование Устройство virtio-gpu (виртуальный GPU), развиваемое в рамках проекта Virgil, расширено поддержкой 3D-операций, что позволит задействовать OpenGL и средства 3D-ускорения в виртуальных окружениях на базе QEMU и KVM без эксклюзивного проброса видеокарты в гостевую систему. Virtio-gpu позволяет организовать 3D-рендеринг внутри гостевых систем с задействованием GPU хост-системы, но при этом виртуальный GPU работает независимо от физического GPU хост-системы; Добавлена подсистема для поддержки устройств широтно-импульсной модуляции (PWM, Pulse-width modulator) и реализована поддержка PWM-контроллеров Renesas R-Car, Marvell Berlin, Broadcom BCM7038 и MediaTek PWM; Добавлен KMS-драйвер vc4 с поддержкой GPU Broadcom VideoCore 4, используемых в Raspberry Pi. Драйвер ограничен переключением видеорежимов на уровне ядра и управлении курсором, но пока не поддерживает 3D и управление питанием; Расширены возможности DRM-драйвера для видеокарт Intel: поддержка HPD (Hot Plug Detect) и загрузчик прошивок, специфичных для движков GuC; Расширены возможности DRM-драйвера для видеокарт NVIDIA (Nouveau): улучшены средства управления частотой GPU, расширена поддержка GPU GK20A (Kepler) и GK107 (GeForce 600); Расширены возможности DRM-драйвера для видеокарт AMD (Radeon и amdgpu): поддержка платформы AMD Stoney Ridge. В amdgpu по умолчаннию включён планировщик GPU, улучшена поддержка GPU AMD Carrizo, Tonga и Fiji, реализованы новые опкоды AtomBIOS; Поддержка SoC Broadcom Northstar Plus; Поддержка сетевых адаптеров: Texas Instruments DP83848, Hisilicon Network, Allwinner A10 CAN, Broadcom Cygnus, Broadcom NetXtreme-C/E 10/25/40/50 gigabit Ethernet, Microchip ENC424J600 ethernet, Mellanox Technologies Spectrum Ethernet, QLogic QED 25/40/100Gb Ethernet, Realtek RTL8XXXU, Intel Fields Peak NFC и Marvell NFC-over-I2C/SPI.