Выпуск системы машинного обучения TensorFlow 2.0

Представлен значительный выпуск платформы машинного обучения TensorFlow 2.0, предоставляющей готовые реализации различных алгоритмов глубокого машинного обучения, простой программный интерфейс для построения моделей на языке Python и низкоуровневый интерфейс для языка С++, позволяющий управлять построением и выполнением вычислительных графов. Код системы написан на языках С++ и Python и распространяется под лицензией Apache. Платформа изначально разработана командой Google Brain и используются в сервисах Google для распознавания речи, выделения лиц на фотографиях, определение схожести изображений, отсеивание спама в Gmail, подбора новостей в Google News и организации перевода с учётом смысла. Распределённые системы машинного обучения можно создавать на типовом оборудовании, благодаря встроенной поддержке в TensorFlow разнесения вычислений на несколько CPU или GPU. TensorFlow предоставляет библиотеку готовых алгоритмов численных вычислений, реализованных через графы потоков данных (data flow graphs). Узлы в таких графах реализуют математические операции или точки входа/вывода, в то время как рёбра графа представляют многомерные массивы данных (тензоры), которые перетекают между узлами. Узлы могут быть закреплены за вычислительными устройствами и выполняться асинхронно, параллельно обрабатывая разом все подходящие к ним тезоры, что позволяет организовать одновременную работу узлов в нейронной сети по аналогии с одновременной активацией нейронов в мозге. Основное внимание при подготовке новой версии было уделено упрощению и простоте использования. Некоторые новшества: Для построения и тренировки моделей предложен новый высокоуровневый API Keras, предоставляющий несколько вариантов интерфейсов для построения моделей (Sequential, Functional, Subclassing) с возможностью их незамедлительного выполнения (без предварительной компиляции) и с простым механизмом отладки; Добавлен API tf.distribute.Strategy для организации распределённого обучения моделей с минимальным изменением существующего кода. Помимо возможности разнесения вычислений на несколько GPU, доступна экспериментальная поддержка разнесения процесса обучения на несколько независимых обработчиков и возможность задействования облачных TPU (Tensor processing unit); Вместо декларативной модели построения графа с выполнением через tf.Session предоставлена возможность написания обычных функций на языке Python, которые при помощи вызова tf.function могут быть преобразованы в графы и затем удалённо выполнены, сериализированы или оптимизированы для повышения производительности; Добавлен транслятор AutoGraph, преобразующий поток команд Python в выражения TensorFlow, что позволяет использовать код на языке Python внутри функций tf.function-decorated, tf.data, tf.distribute и tf.keras; В SavedModel унифицирован формат обмена моделями и добавлена поддержка сохранения и восстановления состояния моделей. Собранные для TensorFlow модели теперь могут быть использованы в TensorFlow Lite (на мобильных устройствах), TensorFlow JS (в браузере или Node.js), TensorFlow Serving и TensorFlow Hub; Унифицированы API tf.train.Optimizers и tf.keras.Optimizers, вместо compute_gradients для вычисления градиентов предложены новый класс GradientTape; Значительно увеличена производительность при использовании GPU. Скорость обучения моделей на системах с GPU NVIDIA Volta и Turing возросла до трёх раз; Проведена большая чистка API, многие вызовы переименованы или удалены, прекращена поддержка глобальных переменных во вспомогательных методах. Вместо tf.app, tf.flags, tf.logging предложен новый API absl-py. Для продолжения использования старого API подготовлен модуль compat.v1.