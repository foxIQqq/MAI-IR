Обновление кластерной файловой системы LizardFS 3.13.0-rc2

После годичного затишья в разработке возобновилась работа над новой веткой отказоустойчивой распределённой файловой системы LizardFS 3.13 и опубликован второй кандидат в релизы. Недавно произошла смена владельцев компании, развивающей LizardFS, было принято новое руководство, и сменились разработчики. Последние два года проект отстранился от сообщества и не уделял ему должного внимания, но новая команда намерена возродить прежние отношения с сообществом и наладить с ним тесное взаимодействие. Код проекта написан на языках С и С++ и распространяется под лицензией GPLv3. LizardFS является распределённой кластерной файловой системой, позволяющей рассредоточить данные по разным серверам, но представить к ним доступ в форме единого большого раздела, работа с которым осуществляется по аналогии с традиционными дисковыми разделами. В монтируемом разделе с LizardFS поддерживаются POSIX-атрибуты файлов, ACL, блокировки, сокеты, каналы, файлы устройств, символические и жёсткие ссылки. Система не имеет единой точки отказа, все компоненты резервируются. Поддерживается распараллеливание операций с данными (несколько клиентов могут одновременно обращаться к файлам). Для обеспечения отказоустойчивости данные разбиваются на реплики, которые распределяются по разным узлам с резервированием (на разных узлах размещается несколько копий), - в случае выхода из строя узлов или накопителей, система продолжает работу без потери информации и автоматически перераспределяет данные с учётом оставшихся узлов. Для расширения хранилища достаточно подключить к нему новые узлы без остановки работы на обслуживание (система сама реплицирует часть данных на новые серверы и сбалансирует хранилище с учётом новых серверов). Аналогично можно поступить и для сокращения размера кластера - можно просто отключить выводимое из состава устаревшее оборудование. Данные и метаданные хранятся раздельно. Для работы рекомендуется устанавливать два сервера метаданных, работающих в режиме master-slave, и а также как минимум два сервера хранения данных (chunkserver). Дополнительно для резервирования метаданных могут применяться лог-серверы, хранящие информацию об изменении метаданных и позволяющие восстановить работу в случае повреждения всех имеющихся серверов метаданных. Каждый файл разбивается на блоки (chunk), размером до 64 МБ. Блоки распределяются по серверам хранения в соответствии с выбранным режимом репликации: стандартный (явное определение числа копий для размещения на разных узлах, в том числе в привязке к отдельным каталогам - для важных данных число копий можно увеличить, а для несущественных уменьшить), XOR (RAID5) и EC (RAID6). Хранилище может масштабироваться до петабайтных размеров. Из областей применения упоминаются ведение архивов, хранение образов виртуальных машин, мультимедийных данных, резервных копий, использование как DRC (Disaster Recovery Center) и как хранилище в кластерах высокопроизводительных вычислений. LizardFS обеспечивает очень высокую скорость чтения файлов любого размера и при записи показывает хорошую производительность при записи целиком больших и средних файлов, когда нет постоянной модификации, интенсивной работы с открытыми файлами и разовых операций с кучей мелких файлов. Среди особенностей ФС также можно отметить наличие поддержки снапшотов, отражающих состояние файлов в определённое время, и встроенную реализацию "корзины" (файлы не удаляются сразу и какое-то время доступны для восстановления). Доступ к разделу может быть ограничен по IP-адресу или паролю (по аналогии с NFS). Имеются механизмы квот и управления качеством сервиса, позволяющие ограничить размер и пропускную способность для некоторых категорий пользователей. Возможно создание территориально распределённых хранилищ, сегменты которых размещены в разных датацентрах. Проект LizardFS был основан в 2013 году как форк MooseFS, и отличается, главным образом, наличием режима репликации на базе кодов коррекции ошибок Рида—Соломона (аналог raidzN), расширенной поддержкой ACL, наличием клиента для платформы Windows, дополнительными оптимизациями (например, при совмещении клиента и сервера хранения блоки по возможности отдаются с текущего узла, а метаданные кэшируются в памяти), более гибкой системой настройки, поддержкой упреждающего чтения данных, квотами на каталоги и внутренними переработками. Релиз LizardFS 3.13.0 планируется выпустить в конце декабря. Основным новшеством LizardFS 3.13 является задействование для обеспечения отказоустойчивости (переключения master-серверов в случае сбоя) алгоритма достижения консенсуса Raft (используется собственная реализация uRaft, которая ранее применялась в коммерческих продуктах). Использование uRaft упрощает настройку и сокращает задержки при восстановлении после сбоя, но требует наличие как минимум трёх работающих узлов, один из которых используется для кворума. Из других изменений: новый клиент на базе подсистемы FUSE3, решение проблем с корректировкой ошибок, плагин nfs-ganesha переписан на языке Си. В обновлении 3.13.0-rc2 исправлено несколько критических ошибок, делавших предыдущие тестовые выпуски ветки 3.13 малопригодными для использования (исправления для ветки 3.12 пока не опубликованы, а обновление с 3.12 на 3.13 по-прежнему приводит к полной потере данных). В 2020 году работа будет сосредоточена на разработке Agama, нового полностью переписанного ядра LizardFS, которое, по заверению разработчиков, обеспечит увеличение производительности в три раза, по сравнению с веткой 3.12. В Agama будет осуществлён переход на событийно-ориентированную архитектуру (event driven), асинхронный ввод/вывод на базе asio, работу преимущественно в пространстве пользователя (для снижения зависимости от механизмов кэширования ядра). Дополнительно будут предложены новая отладочная подсистема и анализатор сетевой активности с поддержкой автотюнинга производительности. В клиент LizardFS будет добавлена полная поддержка версионирования операций записи, которая повысит надёжность восстановления после сбоя, решит проблемы, возникающие при совместном доступе разных клиентов к одним данным, и позволит добиться значительного увеличения производительности. Клиент будет переведён на собственную сетевую подсистему, работающую в пространстве пользователя. Первый рабочий прототип LizardFS на базе Agama планируется подготовить во втором квартале 2020 года. В это же время обещают реализовать средства для интеграции LizardFS с платформой Kubernetes.