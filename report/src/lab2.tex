\section{Поисковый робот}

\subsection{Описание}
Поисковый робот — это компонент, который отвечает за регулярное и корректное наполнение корпуса документами. В учебном проекте робот реализован как Python-утилита с модульной структурой, где каждая функциональная часть вынесена в отдельную функцию для удобства тестирования и замены. Робот должен уметь корректно управлять очередью обхода, учитывать правила доступа (\code{robots.txt}), обрабатывать ошибки сети и сохранять результаты в стандартизированном формате. Важная часть — логирование и мониторинг: робот фиксирует успешные и неуспешные попытки, что позволяет анализировать причины потерь документов и улучшать эвристику фильтрации. Политика politeness встроена в ядро робота, поэтому он не будет посылать много запросов к одному домену за короткий промежуток времени.

\subsection{Архитектура и взаимодействие компонентов}
Архитектура робота строится по принципу «одно назначение — один модуль»: менеджер очереди, проверка robots, загрузчик, парсер, фильтр и модуль хранения. Менеджер очереди отвечает также за дедупликацию URL и контроль глубины обхода; это убирает проблему бесконечных циклов. Модуль проверки robots кеширует парсеры правил, чтобы избежать лишних сетевых вызовов, а загрузчик управляет сессией HTTP и повторными попытками. Парсинг HTML и очистка текста выполняются в отдельном модуле, что даёт возможность легко заменить алгоритм выделения основного текста (например, подключить Readability). Модуль хранения отвечает за конвертацию в UTF-8 и безопасное сохранение на диск с резервным логом.

\subsection{Реализация и инженерные детали}
Реализация ориентирована на надёжность: для каждого домена хранится таймер последней загрузки, что позволяет держать паузу между запросами. Загрузчик использует заголовок User-Agent, идентифицирующий проект, и умеет обрабатывать редиректы и chunked-ответы. Для оптимизации при долгих запусках реализовано сохранение состояния очереди и списка посещённых URL в файл; это позволяет возобновлять работу с места остановки без повторной загрузки уже обработанных страниц. Данные о скачанных страницах и проблемах сохраняются в логах с отметкой времени и кодом ошибки.

\subsection{Тестирование и контроль}
При тестировании проверяется поведение робота на страницах с разными типами контента, на сайтах с ограничениями robots, а также реакции на ошибки сети. Тестовые сценарии включают проверку: корректности парсинга HTML, выделения текста, соблюдения задержек, корректности сохранения файлов и возможности восстановления состояния после аварии. Для контроля качества рекомендуется запускать робота сначала на небольшом наборе сайтов и ручной проверкой подтверждать корректность извлечённого текста, после чего масштабировать сбор на основной корпус.

\subsection{Диаграмма потока (повторно)} 
\begin{center}
\begin{tikzpicture}[node distance=12mm, every node/.style={font=\small}]
  \node[draw, fill=blue!6, rounded corners] (A) {URL from seeds};
  \node[draw, right=of A, fill=blue!6, rounded corners] (B) {robots.txt check};
  \node[draw, right=of B, fill=blue!6, rounded corners] (C) {HTTP Fetch};
  \node[draw, right=of C, fill=blue!6, rounded corners] (D) {HTML Parser};
  \node[draw, below=of D, fill=blue!6, rounded corners] (E) {Text Filter};
  \node[draw, below=of E, fill=blue!6, rounded corners] (F) {Store to disk};

  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C) -- (D);
  \draw[->] (D) -- (E);
  \draw[->] (E) -- (F);
  \draw[->,dashed] (D) -- ++(0,10mm) -| node[pos=0.3,above]{found links} (A);
\end{tikzpicture}
\end{center}

\pagebreak
