\section{Токенизация}

\subsection{Описание}
Токенизация превращает текст в набор лексических токенов, на которых далее базируются стемминг и индексирование. Важной задачей здесь является корректная работа с кодировками и сохранение смысловой единицы (слова) в виде токена, пригодного для стеммера. В реализованном модуле токенизация состоит из нормализации регистра, фильтрации нежелательных символов и разбиения по пробелам. В простейшей конфигурации все небуквенно-цифровые символы заменяются на пробелы, что обеспечивает детерминированное поведение на большинстве страниц. Для специализированных корпусов можно расширять правила допустимых символов (дефисы, точки в номерах версий и т.п.), но это делается локально, не ломая общую логику. Токенизация интегрирована с логикой тестирования, где проверяется поведение на наборе контрольных строк.

\subsection{Архитектура и интерфейс}
Токенизатор реализован как функция с чистым интерфейсом: на вход подаётся строка в UTF-8, на выходе — динамический массив токенов в формате \code{SchVector<SchString>}. Такой интерфейс позволяет легко подменять внутренние правила без изменения остального кода. Важная деталь — возврат оригинальной позиции токена в тексте (offset) при необходимости отображения сниппетов на выдаче. Алгоритм работает линейно по длине текста, что обеспечивает хорошую производительность при больших документах.

\subsection{Реализация и тонкая настройка}
Реализация использует простые и быстрые операции над байт-строкой: проверка isalnum, tolower, запись в промежуточный буфер и последующее чтение через stringstream или ручной парсинг. Минимальная длина токена настраиваема, также есть возможность подключить стоп-лист и фильтр служебных символов. Для производительных сценариев вместо stringstream предпочтительнее использовать ручной разбор, что снижает расходы на аллокации и копирования. В проекте по умолчанию разрешено использовать std::string только во внутренней области токенизации, затем данные переводятся в \code{SchString} для ядра индекса.

\subsection{Тестирование}
Тесты токенизатора включают набор входных строк с разной пунктуацией, смешанными кодировками и техническими маркерами. Проверяется корректность количества токенов, соответствие ожидаемым лексемам и устойчивость к краевым случаям (пустой ввод, строки с одной буквой, длинные последовательности спецсимволов). Тесты автоматизированы и включены в тестовый сценарий проекта, что упрощает контроль регрессий после изменения правил.
\pagebreak
