\section{Добыча корпуса документов}

\subsection{Описание}
Этап добычи корпуса предназначен для формирования единого набора документов, который будет использоваться дальше для индексации и тестов качества поиска. На практике корпус должен представлять собой структурированный набор текстовых файлов в кодировке UTF-8, каждый файл при этом — отдельный документ с явным заголовком и телом. В нашей реализации краулер ориентирован на получение больших тематических коллекций, при этом уделяется внимание одинаковой кодировке всех файлов, надёжному логированию и возможности возобновления работы после сбоев. Важным требованием является соблюдение прав сайта и корректная обработка \code{robots.txt}; краулер проверяет правила доступа и кеширует их для каждого домена. Кроме этого реализована механика politeness — паузы между запросами к одному домену для снижения нагрузки. Для реальной работы необходимо заранее определить пороги по минимальному количеству слов в документе, чтобы отсечь короткие страницы и шум.

\subsection{Архитектура решения}
Архитектура краулера делится на логические модули: менеджер очереди, загрузчик HTTP, парсер HTML, модуль выделения основного текста и модуль хранения файлов. Менеджер очереди обеспечивает устойчивое хранение списка URL и состояний обработки; при масштабировании очередь можно вынести в внешнюю СУБД или систему сообщений. Загрузчик выполняет HTTP-запросы с поддержкой таймаутов и повтора при ошибках; он также анализирует заголовки и кодировки ответа. Парсер HTML находит основные семантические теги и применяет эвристики для выбора основного текстового блока; этот этап критичен для качества корпуса. Файловый модуль сохраняет документы в стандартизированном виде и фиксирует meta-данные (исходный URL, время загрузки). Важная деталь — способность периодически сохранять состояние очереди и списка посещённых URL, что даёт возможность безопасно возобновлять процесс после сбоев.

\subsection{Реализация}
Реализация выполнена в виде Python-скрипта, использующего requests и BeautifulSoup для загрузки и парсинга. Код организован в виде функций с чёткими контрактами: fetch(url) \texttt{->} \texttt{raw\_html}, \texttt{extract\_text(html)} \texttt{->} \texttt{cleaned\_text}, \texttt{save\_document(id, title, text)} и т.д. Для выхода из повторяющихся ошибок реализована стратегия экспоненциального бэкоффа, а также максимальное число попыток для одного URL. Для совместимости с разными сайтами предусмотрены эвристики определения кодировки, а затем чистая конвертация в UTF-8 перед сохранением. Для минимизации коллизий имён файлов применяется порядковая нумерация с префиксом \code{doc\_}. При масштабировании полезно добавить параллельные воркеры с разделением по доменам, чтобы не нарушать politeness.

\subsection{Тестирование и контроль качества}
Тестирование включает несколько уровней: unit-тесты для функций парсинга, интеграционные тесты, имитирующие скачивание набора страниц, и прогон по небольшой тестовой коллекции с ручной проверкой результатов. В тестах проверяется корректность кодировки, отсутствие пустых файлов, соблюдение минимального порога слов и корректность сохранённых meta-данных. Также проверяется устойчивость к ошибкам сети: сценарии с долгим тайм-аутом, 5xx ответами и редиректами. Результаты тестирования фиксируются в лог-файле, в котором указываются URL, статус обработки и причина пропуска при наличии.

\subsection{Диаграмма потока краулера}
\begin{center}
\begin{tikzpicture}[node distance=10mm, every node/.style={font=\small}]
  \node[draw, rounded corners, fill=gray!10] (seeds) {Seeds (seeds.txt)};
  \node[draw, rounded corners, right=20mm of seeds, fill=gray!10] (queue) {Queue};
  \node[draw, rounded corners, right=20mm of queue, fill=gray!10] (fetch) {Fetcher (HTTP)};
  \node[draw, rounded corners, right=20mm of fetch, fill=gray!10] (parser) {HTML Parser};
  \node[draw, rounded corners, below=14mm of parser, fill=gray!10] (filter) {Filter (min words)};
  \node[draw, rounded corners, below=14mm of filter, fill=gray!10] (store) {Storage (data/corpus)};
  \draw[->, thick] (seeds) -- (queue);
  \draw[->, thick] (queue) -- (fetch);
  \draw[->, thick] (fetch) -- (parser);
  \draw[->, thick] (parser) -- (filter);
  \draw[->, thick] (filter) -- (store);
  \draw[->, dashed] (parser) to[bend left] node[above]{extract links} (queue);
\end{tikzpicture}
\end{center}

\pagebreak
